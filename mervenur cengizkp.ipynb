{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import Word2Ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f33870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli NLTK kaynaklarını indirilmesi \n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cd30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sütunlar: ['Text#', 'Type', 'Issued', 'Title', 'Language', 'Authors', 'Subjects', 'LoCC', 'Bookshelves']\n",
      "\n",
      "Verinin ilk 500 karakteri:\n",
      "The Declaration of Independence of the United States of America\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Başlık satırını otomatik olarak kullan\n",
    "df = pd.read_csv('pg_catalog.csv', encoding='utf-8')\n",
    "\n",
    "# Sütun adlarını göster (emin olmak için)\n",
    "print(\"Sütunlar:\", df.columns.tolist())\n",
    "\n",
    "# 'Title' sütununu listeye çevir\n",
    "texts = df['Title'].dropna().astype(str).tolist()\n",
    "\n",
    "# İlk metnin ilk 500 karakterini yazdır\n",
    "if texts:\n",
    "    print(\"\\nVerinin ilk 500 karakteri:\")\n",
    "    print(texts[0][:500])\n",
    "else:\n",
    "    print(\"Veri listesi boş.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e04d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk 10 cümle:\n",
      "['The Declaration of Independence of the United States of America', 'The United States Bill of Rights\\r\\nThe Ten Original Amendments to the Constitution of the United States', \"John F. Kennedy's Inaugural Address\", \"Lincoln's Gettysburg Address\\r\\nGiven November 19, 1863 on the battlefield near Gettysburg, Pennsylvania, USA\", 'The United States Constitution', 'Give Me Liberty or Give Me Death', 'The Mayflower Compact', \"Abraham Lincoln's Second Inaugural Address\", \"Abraham Lincoln's First Inaugural Address\", 'The King James Version of the Bible']\n"
     ]
    }
   ],
   "source": [
    "# [3] Cümlelere ayırma ve ilk 10 cümleyi gösterme\n",
    "sentences = []\n",
    "for text in texts:\n",
    "    if isinstance(text, str):\n",
    "        sentences.extend(sent_tokenize(text))\n",
    "    else:\n",
    "        continue\n",
    "print(\"\\nİlk 10 cümle:\")\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a032660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Türkçe Stopwords (ilk 50):\n",
      "['mı', 'için', 'mu', 'her', 'bu', 'ki', 'mü', 'kadar', 'o', 'de', 'az', 'şu', 'veya', 'ne', 'tüm', 'ya', 'çok', 'mi', 'ise', 'değil', 'ile', 'da', 'hangi', 'niye', 've', 'nasıl', 'ama', 'bir']\n"
     ]
    }
   ],
   "source": [
    "# [4] Türkçe stopwords listesini alma ve ilk 50’sini yazdırma\n",
    "turkish_stopwords = set([\n",
    "    've', 'ile', 'de', 'da', 'ki', 'kadar', 'için', 'ama', 'ya', 'veya',\n",
    "    'bir', 'bu', 'şu', 'o', 'ne', 'nasıl', 'niye', 'hangi', 'her', 'tüm',\n",
    "    'mi', 'mı', 'mu', 'mü', 'ise', 'değil', 'çok', 'az'\n",
    "])\n",
    "stop_words_list = list(turkish_stopwords)\n",
    "print(\"\\nTürkçe Stopwords (ilk 50):\")\n",
    "print(stop_words_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b83c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5] Stemleme fonksiyonunu başlatma\n",
    "def simple_turkish_stem(token):\n",
    "    suffixes = ['ler', 'lar', 'in', 'ın', 'un', 'ün', 'de', 'da', 'ki', 'e', 'a']\n",
    "    for suffix in suffixes:\n",
    "        if token.endswith(suffix):\n",
    "            return token[:-len(suffix)]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d35ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6] Ön işleme fonksiyonunu tanımlama\n",
    "def preprocess_sentence(sentence):\n",
    "    if not isinstance(sentence, str):\n",
    "        return [], []\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in turkish_stopwords]\n",
    "    lemmatized_tokens = filtered_tokens  # Zembereksiz: filtrelenmiş kelimeler\n",
    "    stemmed_tokens = [simple_turkish_stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f7ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7-8] Cümleleri işleme (fonksiyonlu)\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b7c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk İşlenen Cümle (Fonksiyonlu):\n",
      "Ham: The Declaration of Independence of the United States of America\n",
      "Lemmatized: ['the', 'declaration', 'of', 'independence', 'of', 'the', 'united', 'states', 'of', 'america']\n",
      "Stemmed: ['th', 'declaration', 'of', 'independenc', 'of', 'th', 'united', 'states', 'of', 'americ']\n"
     ]
    }
   ],
   "source": [
    "# İlk işlenen cümlenin çıktısını göster\n",
    "print(\"\\nİlk İşlenen Cümle (Fonksiyonlu):\")\n",
    "print(f\"Ham: {sentences[0]}\")\n",
    "print(f\"Lemmatized: {tokenized_corpus_lemmatized[0]}\")\n",
    "print(f\"Stemmed: {tokenized_corpus_stemmed[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fb6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9] Lemmatize edilmiş cümleleri CSV’ye kaydetme\n",
    "with open(\"lemmatized_sentences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62860536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [10] Stemlenmiş cümleleri CSV’ye kaydetme\n",
    "with open(\"stemmed_sentences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5f9dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk 5 Cümle Karşılaştırması (Fonksiyonlu):\n",
      "Cümle 1 - Ham: The Declaration of Independence of the United States of America\n",
      "Cümle 1 - Lemmatized: ['the', 'declaration', 'of', 'independence', 'of', 'the', 'united', 'states', 'of', 'america']\n",
      "Cümle 1 - Stemmed: ['th', 'declaration', 'of', 'independenc', 'of', 'th', 'united', 'states', 'of', 'americ']\n",
      "\n",
      "Cümle 2 - Ham: The United States Bill of Rights\r\n",
      "The Ten Original Amendments to the Constitution of the United States\n",
      "Cümle 2 - Lemmatized: ['the', 'united', 'states', 'bill', 'of', 'rights', 'the', 'ten', 'original', 'amendments', 'to', 'the', 'constitution', 'of', 'the', 'united', 'states']\n",
      "Cümle 2 - Stemmed: ['th', 'united', 'states', 'bill', 'of', 'rights', 'th', 'ten', 'original', 'amendments', 'to', 'th', 'constitution', 'of', 'th', 'united', 'states']\n",
      "\n",
      "Cümle 3 - Ham: John F. Kennedy's Inaugural Address\n",
      "Cümle 3 - Lemmatized: ['john', 'kennedy', 'inaugural', 'address']\n",
      "Cümle 3 - Stemmed: ['john', 'kennedy', 'inaugural', 'address']\n",
      "\n",
      "Cümle 4 - Ham: Lincoln's Gettysburg Address\r\n",
      "Given November 19, 1863 on the battlefield near Gettysburg, Pennsylvania, USA\n",
      "Cümle 4 - Lemmatized: ['lincoln', 'gettysburg', 'address', 'given', 'november', 'on', 'the', 'battlefield', 'near', 'gettysburg', 'pennsylvania', 'usa']\n",
      "Cümle 4 - Stemmed: ['lincoln', 'gettysburg', 'address', 'given', 'november', 'on', 'th', 'battlefield', 'near', 'gettysburg', 'pennsylvani', 'us']\n",
      "\n",
      "Cümle 5 - Ham: The United States Constitution\n",
      "Cümle 5 - Lemmatized: ['the', 'united', 'states', 'constitution']\n",
      "Cümle 5 - Stemmed: ['th', 'united', 'states', 'constitution']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [11] İlk 5 cümlenin ham, lemmatize ve stemlenmiş hallerini yazdırma\n",
    "print(\"\\nİlk 5 Cümle Karşılaştırması (Fonksiyonlu):\")\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"Cümle {i+1} - Ham: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45a181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [16-17] Ayrıntılı for döngüsü ile kelimeleri tokenleştirme ve filtreleme\n",
    "filtered_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            token_lower = token.lower()\n",
    "            if token_lower not in turkish_stopwords:\n",
    "                filtered_tokens.append(token_lower)\n",
    "    filtered_sentences.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4efd0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [18] Lemmatize edilmiş cümleleri oluşturma \n",
    "tokenized_corpus_lemmatized_detailed = filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9524adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ayrıntılı Stemlenmiş Cümleler (ilk 10):\n",
      "[['th', 'declaration', 'of', 'independenc', 'of', 'th', 'united', 'states', 'of', 'americ'], ['th', 'united', 'states', 'bill', 'of', 'rights', 'th', 'ten', 'original', 'amendments', 'to', 'th', 'constitution', 'of', 'th', 'united', 'states'], ['john', 'kennedy', 'inaugural', 'address'], ['lincoln', 'gettysburg', 'address', 'given', 'november', 'on', 'th', 'battlefield', 'near', 'gettysburg', 'pennsylvani', 'us'], ['th', 'united', 'states', 'constitution'], ['giv', 'm', 'liberty', 'or', 'giv', 'm', 'death'], ['th', 'mayflower', 'compact'], ['abraham', 'lincoln', 'second', 'inaugural', 'address'], ['abraham', 'lincoln', 'first', 'inaugural', 'address'], ['th', 'king', 'james', 'version', 'of', 'th', 'bibl']]\n"
     ]
    }
   ],
   "source": [
    "# [19] Ayrıntılı stemleme\n",
    "tokenized_corpus_stemmed_detailed = []\n",
    "for filtered_tokens in filtered_sentences:\n",
    "    stemmed_tokens = [simple_turkish_stem(token) for token in filtered_tokens]\n",
    "    tokenized_corpus_stemmed_detailed.append(stemmed_tokens)\n",
    "print(\"\\nAyrıntılı Stemlenmiş Cümleler (ilk 10):\")\n",
    "print(tokenized_corpus_stemmed_detailed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acafb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [20] Ayrıntılı stemlenmiş cümleleri CSV’ye kaydetme\n",
    "with open(\"stemmed_sentences_detailed.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tokens in tokenized_corpus_stemmed_detailed:\n",
    "        writer.writerow([' '.join(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6033821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk 5 Cümle Karşılaştırması (Ayrıntılı):\n",
      "Cümle 1 - Ham: The Declaration of Independence of the United States of America\n",
      "Cümle 1 - Lemmatized: ['the', 'declaration', 'of', 'independence', 'of', 'the', 'united', 'states', 'of', 'america']\n",
      "Cümle 1 - Stemmed: ['th', 'declaration', 'of', 'independenc', 'of', 'th', 'united', 'states', 'of', 'americ']\n",
      "\n",
      "Cümle 2 - Ham: The United States Bill of Rights\r\n",
      "The Ten Original Amendments to the Constitution of the United States\n",
      "Cümle 2 - Lemmatized: ['the', 'united', 'states', 'bill', 'of', 'rights', 'the', 'ten', 'original', 'amendments', 'to', 'the', 'constitution', 'of', 'the', 'united', 'states']\n",
      "Cümle 2 - Stemmed: ['th', 'united', 'states', 'bill', 'of', 'rights', 'th', 'ten', 'original', 'amendments', 'to', 'th', 'constitution', 'of', 'th', 'united', 'states']\n",
      "\n",
      "Cümle 3 - Ham: John F. Kennedy's Inaugural Address\n",
      "Cümle 3 - Lemmatized: ['john', 'kennedy', 'inaugural', 'address']\n",
      "Cümle 3 - Stemmed: ['john', 'kennedy', 'inaugural', 'address']\n",
      "\n",
      "Cümle 4 - Ham: Lincoln's Gettysburg Address\r\n",
      "Given November 19, 1863 on the battlefield near Gettysburg, Pennsylvania, USA\n",
      "Cümle 4 - Lemmatized: ['lincoln', 'gettysburg', 'address', 'given', 'november', 'on', 'the', 'battlefield', 'near', 'gettysburg', 'pennsylvania', 'usa']\n",
      "Cümle 4 - Stemmed: ['lincoln', 'gettysburg', 'address', 'given', 'november', 'on', 'th', 'battlefield', 'near', 'gettysburg', 'pennsylvani', 'us']\n",
      "\n",
      "Cümle 5 - Ham: The United States Constitution\n",
      "Cümle 5 - Lemmatized: ['the', 'united', 'states', 'constitution']\n",
      "Cümle 5 - Stemmed: ['th', 'united', 'states', 'constitution']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [21] İlk 5 cümlenin ayrıntılı işlenmiş hallerini yazdırma\n",
    "print(\"\\nİlk 5 Cümle Karşılaştırması (Ayrıntılı):\")\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"Cümle {i+1} - Ham: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized_detailed[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed_detailed[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac68c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk 3 lemmatized metin:\n",
      "['the declaration of independence of the united states of america', 'the united states bill of rights the ten original amendments to the constitution of the united states', 'john kennedy inaugural address']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [2-3] Lemmatize metinleri oluşturma ve ilk 3’ünü gösterme\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "print(\"\\nİlk 3 lemmatized metin:\")\n",
    "print(lemmatized_texts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e21a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4] TF-IDF vektörizasyon\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53752944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk 5 cümlenin TF-IDF skorları:\n",
      "   aab  aabenserraagin  aada  aade  aadolf  aalberg  aalloilta  aalloissa  \\\n",
      "0  0.0             0.0   0.0   0.0     0.0      0.0        0.0        0.0   \n",
      "1  0.0             0.0   0.0   0.0     0.0      0.0        0.0        0.0   \n",
      "2  0.0             0.0   0.0   0.0     0.0      0.0        0.0        0.0   \n",
      "3  0.0             0.0   0.0   0.0     0.0      0.0        0.0        0.0   \n",
      "4  0.0             0.0   0.0   0.0     0.0      0.0        0.0        0.0   \n",
      "\n",
      "   aallonhalkoja  aallot  ...  高士傳  鬼谷四友志  鬼谷子  魏鄭公諫錄  鶯鶯傳  鹽鐵論  麟兒報  黃繡球  \\\n",
      "0            0.0     0.0  ...  0.0    0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "1            0.0     0.0  ...  0.0    0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "2            0.0     0.0  ...  0.0    0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "3            0.0     0.0  ...  0.0    0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "4            0.0     0.0  ...  0.0    0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   黄帝宅經  龍川詞  \n",
      "0   0.0  0.0  \n",
      "1   0.0  0.0  \n",
      "2   0.0  0.0  \n",
      "3   0.0  0.0  \n",
      "4   0.0  0.0  \n",
      "\n",
      "[5 rows x 52372 columns]\n"
     ]
    }
   ],
   "source": [
    "# [5] İlk 5 cümlenin TF-IDF skorlarını yazdırma\n",
    "print(\"\\nİlk 5 cümlenin TF-IDF skorları:\")\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce66f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "declaration     0.512973\n",
      "independence    0.467458\n",
      "of              0.350719\n",
      "states          0.346484\n",
      "united          0.345850\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "print(\"\\nİlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82a07b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'sorun' kelimesi veri setinde bulunamadı.\n"
     ]
    }
   ],
   "source": [
    "# [6] \"sorun\" kelimesi için cosine benzerlik analizi\n",
    "try:\n",
    "    sorun_index = list(feature_names).index('sorun')\n",
    "    sorun_vector = tfidf_matrix[:, sorun_index].toarray()\n",
    "    tfidf_vectors = tfidf_matrix.toarray()\n",
    "    similarities = cosine_similarity(sorun_vector.T, tfidf_vectors.T)\n",
    "    similarities = similarities.flatten()\n",
    "    top_5_indices = similarities.argsort()[-6:][:-1]\n",
    "    print(\"\\n'sorun' kelimesine en benzer 5 kelime:\")\n",
    "    for index in top_5_indices:\n",
    "        print(f\"{feature_names[index]}: {similarities[index]:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"\\n'sorun' kelimesi veri setinde bulunamadı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8824419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word2Vec modeli eğitmek için parametreler\n",
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df86612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonksiyon ile Word2Vec modeli eğitme ve kaydetme\n",
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'],\n",
    "                     window=params['window'], min_count=1,\n",
    "                     sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49340fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c1cfc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100.model saved!\n",
      "lemmatized_model_skipgram_window2_dim100.model saved!\n",
      "lemmatized_model_cbow_window4_dim100.model saved!\n",
      "lemmatized_model_skipgram_window4_dim100.model saved!\n",
      "lemmatized_model_cbow_window2_dim300.model saved!\n",
      "lemmatized_model_cbow_window4_dim300.model saved!\n",
      "lemmatized_model_skipgram_window4_dim300.model saved!\n",
      "stemmed_model_cbow_window2_dim100.model saved!\n",
      "stemmed_model_skipgram_window2_dim100.model saved!\n",
      "stemmed_model_cbow_window4_dim100.model saved!\n",
      "stemmed_model_skipgram_window4_dim100.model saved!\n",
      "stemmed_model_cbow_window2_dim300.model saved!\n",
      "stemmed_model_cbow_window4_dim300.model saved!\n",
      "stemmed_model_skipgram_window4_dim300.model saved!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "    # Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f0447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modellerden biri bulunamadı. Önce modelleri eğitip kaydedin.\n"
     ]
    }
   ],
   "source": [
    "# [17] Üç model yükleme ve \"sorun\" kelimesi için en benzer 3 kelimeyi yazdırma\n",
    "# Model dosyalarını yükleme\n",
    "try:\n",
    "    model_1 = Word2Vec.load(\"lemmatized_model_cbow_window2_dim100.model\")\n",
    "    model_2 = Word2Vec.load(\"stemmed_model_skipgram_window2_dim300.model\")\n",
    "    model_3 = Word2Vec.load(\"lemmatized_model_skipgram_window2_dim300.model\")\n",
    "\n",
    "    # 'sorun' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırma\n",
    "    def print_similar_words(model, model_name):\n",
    "        try:\n",
    "            similarity = model.wv.most_similar('sorun', topn=3)\n",
    "            print(f\"\\n{model_name} Modeli - 'sorun' ile En Benzer 3 Kelime:\")\n",
    "            for word, score in similarity:\n",
    "                print(f\"Kelime: {word}, Benzerlik Skoru: {score:.4f}\")\n",
    "        except KeyError:\n",
    "            print(f\"\\n{model_name} Modeli - 'sorun' kelimesi modelde bulunamadı.\")\n",
    "\n",
    "    # 3 model için benzer kelimeleri yazdırma\n",
    "    print_similar_words(model_1, \"Lemmatized CBOW Window 2 Dim 100\")\n",
    "    print_similar_words(model_2, \"Stemmed Skipgram Window 2 Dim 300\")\n",
    "    print_similar_words(model_3, \"Lemmatized Skipgram Window 2 Dim 300\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nModellerden biri bulunamadı. Önce modelleri eğitip kaydedin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f80d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d20b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67078476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
